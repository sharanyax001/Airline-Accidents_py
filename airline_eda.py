# -*- coding: utf-8 -*-
"""Airline EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgNQ7R7RAgnM5Ukx8TUtP-mzQkJG0hE3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# importing data
data=pd.read_csv('/airline_accidents.csv')
data=data.loc[:150951,:]
data.sample(15)

data[~data['Investigation Type'].isnull()]['Investigation Type']



data['Aircraft Category'].value_counts()

data.shape

data.info()

cols=data.columns
count_null=pd.DataFrame([],columns=["No of null"])
for i in cols:
  null_c=pd.DataFrame(data.loc[data[i]=='  ',i]).shape[0]
  count_null=count_null.append({'No of null':null_c},ignore_index=True)

count_null

data['Investigation Type'].value_counts()

# dropping columns with moe than 92% missing values
cols_todrop=['Latitude','Longitude','Event Id','Air Carrier','Unnamed: 30','Schedule','Airport Code','Airport Name','Report Publication Date']
data=data.drop(cols_todrop,axis=1)

data

data['Make'].value_counts()

data.info()

#function to replace spaces with nan  
def replace_with_nan(j):
    for i in range(data.shape[0]):
       s=data.iloc[i,j]
       if s==' ' or s=='  ':
          data.iloc[i,j]=np.nan

for i in range(22):
  replace_with_nan(i)

data.isnull().sum()

data=data.drop(['FAR Description','Aircraft Category','Location'],axis=1)
data

# Dealing with make
for i in range(data.shape[0]):
  if type(data.loc[i,'Make'])!=float:
     s=data.loc[i,'Make']
     data.loc[i,'Make']=s.upper()

data['Make'].value_counts()

# filling nan with the make with max frequency
data['Make']=data['Make'].fillna('CESSNA')



# making a new feature event year,
import re
yr=pd.DataFrame([],columns=["Event Year"])
for i in range(data.shape[0]):
   s=data.iloc[i,2]
   reg=re.compile(r'\d+/\d+/(\d+)')
   mo=reg.search(s)
   res=mo.group(1)
   yr.loc[i,'Event Year']=res
   
data=data.drop('Event Date',axis=1)
data=pd.concat([data,yr],axis=1)

data['Amateur Built']=data['Amateur Built'].fillna('No')

data

data['Country'].value_counts()

data['Country']=data['Country'].fillna('United States')

data.dropna(thresh=8,inplace=True)
data.shape

data.isnull().sum()

data['Total Uninjured'].value_counts()

data.info()

data.drop('Model',axis=1)

data=data.astype({'Number of Engines':'float64'})

data=data.astype({'Total Uninjured':'float64'})
data=data.astype({'Total Fatal Injuries':'float64'})
data=data.astype({'Total Serious Injuries':'float64'})

data['Total Uninjured']=data['Total Uninjured'].fillna(data['Total Uninjured'].median())

data['Number of Engines']=data['Number of Engines'].fillna(data['Number of Engines'].median())

data=data.astype({'Total Minor Injuries':'float64'})

data['Total Fatal Injuries']=data['Total Fatal Injuries'].fillna(data['Total Fatal Injuries'].median())
data['Total Serious Injuries']=data['Total Serious Injuries'].fillna(data['Total Serious Injuries'].median())
data['Total Minor Injuries']=data['Total Minor Injuries'].fillna(data['Total Minor Injuries'].median())

data=data.drop('Registration Number',axis=1)
data

data.info()

#creating a new feature Degree Of Injury
degree=pd.DataFrame([],columns=['Injuries'])
for i in range(data.shape[0]):
   s=(data.iloc[i,11]+data.iloc[i,12]+data.iloc[i,13])
   degree.loc[i,'Injuries']=s

degree

data=data.drop(['Total Fatal Injuries','Purpose of Flight','Total Serious Injuries','Total Minor Injuries'],axis=1)
data=pd.concat([degree,data],axis=1)

data.isnull().sum()

data=data.drop(['Model'],axis=1)

data

# transforming the Injury Severity feature
data['Injury Severity'].value_counts()

# Making a new Feature ----(No Of Deaths)
deaths=pd.DataFrame([],columns=['No Of Deaths'])
for i in range(data.shape[0]):
   s=data.iloc[i,4]
   if type(s)!=float:
     if 'Fatal' in s and 'Non' not in s:
       reg=re.compile(r'\w+\(?(\d+)?\)?')
       mo=reg.search(s)
       n=mo.group(1)
       deaths.loc[i,'No Of Deaths']=n
       data.iloc[i,4]='F'
     elif 'Non' in s and 'Fatal' in s:
       data.iloc[i,4]='N'
       deaths.loc[i,'No Of Deaths']=0
  
data=pd.concat([deaths,data],axis=1)

data

data['No Of Deaths'].value_counts()

data

data=data.astype({'Event Year':'float64'})
data['Injuries'].value_counts()

data.info()

data=data.astype({'No Of Deaths':'float64','Injuries':'float64'})

data.info()

data['Injury Severity'].value_counts()

data['No Of Deaths']=data['No Of Deaths'].fillna(data['No Of Deaths'].median())

# Assigning value to Investigation Type, use groupby by Injury Severity

data[data['No Of Deaths']>1]['Injury Severity'].value_counts()
# here we can see that if there is a single death then it's categorized as fatal

data[data['Injury Severity']=='F']['Investigation Type'].value_counts()
# And if the Injury is 'Fatal', The type of Investigation would most probably be Accident
# So we could fill the Investigation Type feature based on this aspect

data[data['Injuries']>5]['Injury Severity'].value_counts()
# According to the data, It's death and not the no. of injuries which has a greater control on wheather an event is fatal or not

data['Injury Severity'].value_counts()

for i in range(data.shape[0]):
  x=data.iloc[i,5]
  if type(x)!=float:
    if x=='F':
      data.iloc[i,2]='Accident'
    elif x=='N':
      data.iloc[i,2]='Incident'

data.isnull().sum()

data['Aircraft Damage'].value_counts()

data[data['No Of Deaths']>1]['Amateur Built'].value_counts()

for i in range(data.shape[0]):
  x=data.iloc[i,8]
  if type(x)!=float:
    if 'No' in x:
      data.iloc[i,8]='No'
    elif 'Yes' in x:
      data.iloc[i,8]='Yes'

data['Amateur Built'].value_counts()

data[data['Amateur Built']=='Yes']['Aircraft Damage'].value_counts()
# half of the amateurly bulit ones led to Substantial damage(3700 out of 7400 amateur built)
#we could say that if a engine is built amateurly then there's a 75-80 percent chance of it being damaged
#substantially or being destroyed(5500 out of 7400)

data[data['Engine Type']=='Reciprocating']['Investigation Type']

data[data['Engine Type']=='Reciprocating']['Investigation Type'].value_counts()

data['Country'].value_counts()
for i in range(data.shape[0]):
  s=data.iloc[i,4]
  if type(s)!=float:
    if 'United States' in s:
      data.iloc[i,4]='United States'
    else:
      data.iloc[i,4]='Other'
data['Country'].value_counts()

data[data['Country']=='Other']['Investigation Type'].value_counts()

n_accident_usa=data[data['Country']=='United States']['Investigation Type'].value_counts()[1]
n_accidents=data[data['Investigation Type']=='Accident'].shape[0]
percent_usa=n_accident_usa/n_accidents
n_accident_other=data[data['Country']=='Other']['Investigation Type'].value_counts()[1]
percent_other=n_accident_other/n_accidents

print("Percentage of accidents in the usa= ",(percent_usa*100))
print("          ")
print("Percentage of accidents in other countries",(percent_other*100))

data.info()

data=data.drop(['Broad Phase of Flight','Aircraft Damage'],axis=1)

data[data['Investigation Type']=='Accident']['Engine Type'].value_counts()

# Let's find out the maximum accidents in which weather condition
data[data['Investigation Type']=='Accident']['Weather Condition'].value_counts()
#Most of the accidents are in VMC conditions, but also we can't be absolutely sure as  VMC causes incidents too
# what can we be sure of is that VMC weather conditions affect the vehicles.

data.isnull().sum()

data=data.drop(['Engine Type','Weather Condition'],axis=1)

#univariate analysis
sns.displot(data['Event Year'],bins=20)

data=data.dropna()

data.isnull().sum()

import seaborn as sns

# Incident/accident count vs event year 
fig, axs = plt.subplots(figsize=(22, 9))
sns.countplot(x='Event Year', hue='Investigation Type', data=data)

plt.xlabel('Year', size=15, labelpad=20)
plt.ylabel('Incident/Accident Count', size=15, labelpad=20)
plt.tick_params(axis='x', labelsize=10)
plt.tick_params(axis='y', labelsize=15)

# Countplot for accident vs inference in the united states 

fig, axs = plt.subplots(figsize=(22, 9))
sns.countplot(x='Country', hue='Investigation Type', data=data)

plt.xlabel('Country', size=15, labelpad=20)
plt.ylabel('Incident/Accident Count', size=15, labelpad=20)
plt.tick_params(axis='x', labelsize=10)
plt.tick_params(axis='y', labelsize=15)

# Countplot for the relationship between Amateur Built and Investigation Type
fig, axs = plt.subplots(figsize=(22, 9))
sns.countplot(x='Amateur Built', hue='Investigation Type', data=data)

plt.xlabel('Amateur built', size=15, labelpad=20)
plt.ylabel('Incident/Accident Count', size=15, labelpad=20)
plt.tick_params(axis='x', labelsize=10)
plt.tick_params(axis='y', labelsize=15)

# It turns out that Amateur built engines are not responsible for accidents

data=data.drop(['Accident Number'],axis=1)

data.head(10)

# finding correlation 
data_corr=data.corr().abs()
data_corr

sns.heatmap(data_corr,annot=True)

data.boxplot(column=['No Of Deaths','Injuries','Event Year','Total Uninjured'])

sns.pairplot(data)

"""Inferences"""

#1)Amateur built engines are not responsible for accidents

#2)Mostly thhe injuries were not fatal, that is number of accident< no. of incidents

#3)#Most of the accidents are in VMC conditions, VMC causes many incidents too 
   #what can we be sure of is that VMC weather conditions affect the vehicles 

#4)half of the amateurly bulit ones led to Substantial damage(3700 out of 7400 amateur built)
   #we could say that if a engine is built amateurly then there's a 75-80 percent chance of it being damaged
   #substantially or being destroyed(5500 out of 7400)


#5)And if the Injury is 'Fatal', The type of Investigation would most probably be Accident
   #So we could fill the Investigation Type feature based on this aspect

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay

data

data['Amateur Built'].unique()

hot_encoding = {'Yes' : 1, 'No': 0}
data['Amateur Built'] = data['Amateur Built'].map(hot_encoding)

data['Country'].unique()

hot_encoding = {'United States': 1, 'Other': 0}
data['Country'] = data['Country'].map(hot_encoding)

data['Amateur Built'].unique()

data

X = data.drop(['Investigation Type','Injury Severity','Make'],axis=1)
y=data['Investigation Type']

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

rf = RandomForestClassifier()

rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cm = confusion_matrix(y_test, y_pred)

ConfusionMatrixDisplay(confusion_matrix=cm).plot();

